{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import pandas as pd\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Speech Commands Dataset Download\n",
    "\n",
    "# !wget http://download.tensorflow.org/data/speech_commands_v0.01.tar.gz\n",
    "# ! mkdir speech_commands\n",
    "# ! tar -zxvf speech_commands_v0.01.tar.gz -C ./speech_commands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Audio File\n",
    "\n",
    "## Data : Google Speech Command\n",
    "\n",
    "30개 음성 명령어 데이터 \n",
    "https://huggingface.co/datasets/google/speech_commands\n",
    "\n",
    "### Current Status\n",
    "\n",
    "- version 0.01 : 64,727 recordings (\"Yes\", \"No\", \"Up\", \"Down\", \"Left\", \"Right\", \"On\", \"Off\", \"Stop\", \"Go\", \"Zero\", \"One\", \"Two\", \"Three\", \"Four\", \"Five\", \"Six\", \"Seven\", \"Eight\", \"Nine\", \"Bed\", \"Bird\", \"Cat\", \"Dog\", \"Happy\", \"House\", \"Marvin\", \"Sheila\", \"Tree\", \"Wow\".\")\n",
    "- version 0.02 : 105,829 recordings (version 0.01에 \"Backward\", \"Forward\", \"Follow\", \"Learn\", \"Visual\" 추가)\n",
    "\n",
    "### Supoort Task\n",
    "- Keyword Spotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `datasets` : audio, computer vision, nlp task 용 공유 데이터에 쉽게 접근할 수 있는 라이브러리, huggingface에서 사용됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "train_dataset = load_dataset(\"google/speech_commands\", \"v0.01\", split=\"train\")\n",
    "val_dataset = load_dataset(\"google/speech_commands\", \"v0.01\", split=\"validation\")\n",
    "test_dataset = load_dataset(\"google/speech_commands\", \"v0.01\", split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = train_dataset[0]\n",
    "print(sample)\n",
    "print(\"audio\")\n",
    "print(sample['audio'])\n",
    "print(\"array\")\n",
    "print(sample['audio']['array'])\n",
    "print(\"label\")\n",
    "print(sample['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "\n",
    "sample = train_dataset[0]\n",
    "ipd.Audio(sample['audio']['array'], rate=16000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2FeatureExtractor, Wav2Vec2Model\n",
    "\n",
    "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\"facebook/wav2vec2-base\")\n",
    "model = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = train_dataset[0]\n",
    "print(len(sample['audio']['array']))\n",
    "input = feature_extractor(sample['audio']['array'], sampling_rate=feature_extractor.sampling_rate)\n",
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(feature_extractor)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_values = torch.tensor(input['input_values'])\n",
    "print(input_values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(input_values)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `last_hidden_state`\n",
    "- `extract_features`\n",
    "- `hidden_states` : model(input_values, output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_hidden_state = out['last_hidden_state']\n",
    "extract_features = out['extract_features']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset,DataLoader\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 3\n",
    "\n",
    "train_dataset = train_dataset.filter(lambda x : x['label'] < n_classes)\n",
    "val_dataset = val_dataset.filter(lambda x : x['label'] < n_classes)\n",
    "test_dataset = test_dataset.filter(lambda x : x['label'] < n_classes)\n",
    "\n",
    "print(len(train_dataset), len(val_dataset), len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ds_to_df(dataset):\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    df['file'] = dataset['file']\n",
    "    df['array'] = [x['array'] for x in dataset['audio']]\n",
    "    df['label'] = dataset['label']\n",
    "    df['is_unknown'] = dataset['is_unknown']\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = ds_to_df(train_dataset)\n",
    "val_df = ds_to_df(val_dataset)\n",
    "test_df = ds_to_df(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df = train_df[train_df['label'] < n_classes].reset_index(drop=True)\n",
    "# val_df = val_df[val_df['label'] < n_classes].reset_index(drop=True)\n",
    "# test_df = test_df[test_df['label'] < n_classes].reset_index(drop=True)\n",
    "\n",
    "# print(len(train_df), len(val_df), len(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_df.loc[0])\n",
    "print(\"array\")\n",
    "print(test_df.loc[0, 'array'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Daset Object 개념 \n",
    "`torch.utils.data.Dataset`의 subclass는 `__len__`과 `__getitem__`을 구현하기 위해 필요한 객체이다\n",
    "- `__len__` : 데이터셋의 아이템 수를 반환\n",
    "- `__getitem__`: 샘플과 레이블을 반환\n",
    "\n",
    "\n",
    "[그림]데이터를 직접적으로 가지고 있지 않지만 `__len__` 과 `__getitem__`을 통해 접근가능\n",
    "\n",
    "<img src=\"https://drek4537l1klr.cloudfront.net/stevens2/Figures/CH07_F02_Stevens2_GS.png\" width=600>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataSet(Dataset) : \n",
    "  def __init__(self,df):\n",
    "    self.df = df\n",
    "    self.sr = 16000\n",
    "    self.max_length = self.get_max_length()\n",
    "    self.feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\"facebook/wav2vec2-base\")\n",
    "    \n",
    "  \n",
    "  def get_max_length(self): \n",
    "    max_length = 0\n",
    "    for i, row in tqdm(self.df.iterrows(), total = len(self.df)):\n",
    "      if row['label'] == 30:\n",
    "        continue\n",
    "      \n",
    "      array = row['array']\n",
    "      max_length = max(max_length, len(array))\n",
    "    \n",
    "    return max_length\n",
    "\n",
    "  def __len__(self):    \n",
    "    return len(self.df)\n",
    "\n",
    "  def __getitem__(self,idx):\n",
    "    # audio_path = self.dataset[idx]['file']\n",
    "    # y,sr = torchaudio.load(audio_path)\n",
    "    # padded  = torch.zeros(20000)\n",
    "    # padded[:len(y[0])] = y[0]\n",
    "    \n",
    "    audio = self.df.loc[idx, 'array']\n",
    "    \n",
    "    audio = self.feature_extractor(\n",
    "        audio, sampling_rate=self.feature_extractor.sampling_rate, max_length=self.max_length, truncation=True, padding='max_length', return_tensors=\"pt\"\n",
    "    )\n",
    "    audio_values = audio['input_values'][0]\n",
    "    \n",
    "    label = self.df.loc[idx, \"label\"]\n",
    "    \n",
    "    return {'input_values':audio_values, 'label':label}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = AudioDataSet(train_df)\n",
    "val = AudioDataSet(val_df)\n",
    "test = AudioDataSet(test_df)\n",
    "\n",
    "inputs = test[0]\n",
    "# inputs['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train, shuffle=True, batch_size=8)\n",
    "out = next(iter(train_loader))\n",
    "audio = out['input_values']\n",
    "label = out['label']\n",
    "print(audio)\n",
    "print(audio.shape)\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create id2label dictionary\n",
    "\n",
    "labels = train_dataset.features[\"label\"].names\n",
    "label2id, id2label = dict(), dict()\n",
    "for i, label in enumerate(labels[:n_classes]):\n",
    "    label2id[label] = i\n",
    "    id2label[i] = label\n",
    "\n",
    "id2label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install evaluate\n",
    "! pip install accelerate -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "??Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "??TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Computes accuracy on a batch of predictions\"\"\"\n",
    "    \n",
    "    predictions = np.argmax(eval_pred.predictions, axis=-1)\n",
    "    return accuracy.compute(predictions=predictions, references=eval_pred.label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "from collections.abc import Mapping\n",
    "\n",
    "def nested_detach(tensors):\n",
    "    \"Detach `tensors` (even if it's a nested list/tuple/dict of tensors).\"\n",
    "    if isinstance(tensors, (list, tuple)):\n",
    "        return type(tensors)(nested_detach(t) for t in tensors)\n",
    "    elif isinstance(tensors, Mapping):\n",
    "        return type(tensors)({k: nested_detach(t) for k, t in tensors.items()})\n",
    "    return tensors.detach()\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    \n",
    "    def prediction_step(\n",
    "        self, model, inputs, prediction_loss_only,ignore_keys ) :\n",
    "        labels = inputs['labels']\n",
    "        # _, labels = inputs\n",
    "        \n",
    "        if ignore_keys is None:\n",
    "            if hasattr(self.model, \"config\"):\n",
    "                ignore_keys = getattr(self.model.config, \"keys_to_ignore_at_inference\", [])\n",
    "            else:\n",
    "                ignore_keys = []\n",
    "                \n",
    "        with torch.no_grad():       \n",
    "\n",
    "            loss, outputs = self.compute_loss(model, inputs, return_outputs=True)\n",
    "            loss = loss.mean().detach()\n",
    "\n",
    "            if isinstance(outputs, dict):\n",
    "                logits = tuple(v for k, v in outputs.items() if k not in ignore_keys + [\"loss\"])\n",
    "            else:\n",
    "                logits = outputs\n",
    "            \n",
    "        if prediction_loss_only:\n",
    "            return (loss, None, None)\n",
    "        \n",
    "        labels = nested_detach(labels)\n",
    "        logits = nested_detach(logits)\n",
    "        if len(logits) == 1:\n",
    "            logits = logits[0]\n",
    "\n",
    "        return (loss, logits, labels)\n",
    "    \n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "\n",
    "        audios, labels = inputs['input_values'], inputs['labels']\n",
    "        # forward pass\n",
    "        outputs = model(audios)\n",
    "        outputs = outputs['logits']\n",
    "        # compute custom loss for 3 labels with different weights\n",
    "        labels = labels.type(torch.LongTensor).to('cuda')\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForAudioClassification, TrainingArguments\n",
    "\n",
    "num_labels = len(id2label)\n",
    "ksmodel = AutoModelForAudioClassification.from_pretrained(\n",
    "    \"facebook/wav2vec2-base\", num_labels=num_labels, label2id=label2id, id2label=id2label\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train, shuffle=True, batch_size=8)\n",
    "inputs = next(iter(train_loader))\n",
    "audio, label =inputs['input_values'], inputs['label']\n",
    "out = ksmodel(audio)\n",
    "out = out['logits']\n",
    "\n",
    "label = label.type(torch.LongTensor)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "loss = loss_fn(out, label)\n",
    "loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=3e-5,\n",
    "    num_train_epochs=5,\n",
    "    push_to_hub=False\n",
    ")\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "    model=ksmodel,\n",
    "    args=training_args,\n",
    "    train_dataset=train,\n",
    "    eval_dataset=val,\n",
    "    compute_metrics=compute_metrics)\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.classification import MulticlassAccuracy\n",
    "\n",
    "test_loader = DataLoader(test, batch_size=8)\n",
    "accuracy_metric = MulticlassAccuracy(num_classes=3).to('cuda')\n",
    "\n",
    "test_accuracy = []\n",
    "\n",
    "ksmodel.eval()\n",
    "\n",
    "for batch in test_loader:\n",
    "    audio, label =batch['input_values'].to('cuda'), batch['label'].to('cuda')\n",
    "\n",
    "    out = ksmodel(audio)\n",
    "    out = out['logits']\n",
    "\n",
    "    pred = out.argmax(dim=-1)\n",
    "\n",
    "    test_acc = accuracy_metric(pred, label)\n",
    "    test_accuracy.append(test_acc)\n",
    "    \n",
    "print(f\"test_accuracy : {torch.tensor(test_accuracy).mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels = len(id2label)\n",
    "ksmodel2 = AutoModelForAudioClassification.from_pretrained(\n",
    "    \"facebook/wav2vec2-base\", num_labels=num_labels, label2id=label2id, id2label=id2label\n",
    ")\n",
    "\n",
    "\n",
    "hidden_dim = 256\n",
    "\n",
    "ksmodel2.classifier = nn.Sequential(\n",
    "                          nn.Linear(hidden_dim, hidden_dim),\n",
    "                          nn.ReLU(),\n",
    "                          nn.Linear(hidden_dim, hidden_dim),\n",
    "                          nn.ReLU(),\n",
    "                          nn.Linear(hidden_dim, num_labels))\n",
    "\n",
    "ksmodel2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=3e-5,\n",
    "    num_train_epochs=5,\n",
    "    push_to_hub=False\n",
    ")\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "    model=ksmodel2,\n",
    "    args=training_args,\n",
    "    train_dataset=train,\n",
    "    eval_dataset=val,\n",
    "    compute_metrics=compute_metrics)\n",
    "\n",
    "trainer.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_model = AutoModelForAudioClassification.from_pretrained(\"/mnt/code/asr_wav2vec_tutorial/results/checkpoint-2780\")\n",
    "pretrain_model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
